{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Review Sentiment Anlysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews \n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk import classify \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import ngrams\n",
    "from random import shuffle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the functions for cleaning the data\n",
    "### We created 4 functions to help us clean the data\n",
    "### - clean_words(words, stopwords)\n",
    "This function takes two parameters one of which is the review being classified and the other english stopwords. This function also removes all punctuations using this *string* class\n",
    "### - unigram_words(words)\n",
    "This function is used to create a dictionary of *unigrams*, with the key value being each word excluding the english stopwords and punctuations..\n",
    "### - bigram_words(words, n=2)\n",
    "This function is used to create a dictionary of *bigrams*. We use the *nltk ngrams* import which can be used to create any size of grams. We initialize the (n) as 2 in the beginning to make the ngram into a bigram. And return a dictionary containing two words of the original list as keys. \n",
    "### - clean_all_words(words, n=2)\n",
    "This is the main function to clean the data. It uses all the above functions to *clean* the text, makes into a *unigram* and *bigram* dictionary and lastly it makes a dictionary containing both the unigrams and bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('english')\n",
    " \n",
    "# Remove stopwords and punctuations\n",
    "def clean_words(words, stopwords):\n",
    "    words_clean = []\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if word not in stopwords and word not in string.punctuation:\n",
    "            words_clean.append(word)    \n",
    "    return words_clean \n",
    " \n",
    "# Create unigrams\n",
    "def unigram_words(words):    \n",
    "    words_dictionary = dict([word, True] for word in words)    \n",
    "    return words_dictionary\n",
    " \n",
    "# Create bigrams\n",
    "def bigram_words(words, n=2):\n",
    "    words_bigram = []\n",
    "    for i in iter(ngrams(words, n)):\n",
    "        words_bigram.append(i)\n",
    "    words_dictionary = dict([word, True] for word in words_bigram)    \n",
    "    return words_dictionary\n",
    " \n",
    "# Cleaning all the data\n",
    "def clean_all_words(words):\n",
    "    # Clean text\n",
    "    words_clean = clean_words(words, stopwords)\n",
    "    \n",
    "    # Cleaned text into unigrams\n",
    "    unigram_features = unigram_words(words_clean)\n",
    "    # Cleaned text into bigrams\n",
    "    bigram_features = bigram_words(words_clean)\n",
    " \n",
    "    # Unigrammed text copied as \"all_features\" and add bigrams to list\n",
    "    all_features = unigram_features.copy()\n",
    "    all_features.update(bigram_features)\n",
    " \n",
    "    return all_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our features\n",
    "Here we test all our functions to make sure that they work as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized text: ['this', 'is', 'our', 'test', 'review', 'because', 'we', 'thought', 'the', 'movie', 'was', 'very', 'bad', ',', 'just', 'kidding', 'it', 'was', 'great', '!'] \n",
      "\n",
      "Bigrammed text:  {('this', 'is'): True, ('is', 'our'): True, ('our', 'test'): True, ('test', 'review'): True, ('review', 'because'): True, ('because', 'we'): True, ('we', 'thought'): True, ('thought', 'the'): True, ('the', 'movie'): True, ('movie', 'was'): True, ('was', 'very'): True, ('very', 'bad'): True, ('bad', ','): True, (',', 'just'): True, ('just', 'kidding'): True, ('kidding', 'it'): True, ('it', 'was'): True, ('was', 'great'): True, ('great', '!'): True} \n",
      "\n",
      "Cleaned text:  ['test', 'review', 'thought', 'movie', 'bad', 'kidding', 'great'] \n",
      "\n",
      "All words cleaned:  {'test': True, 'review': True, 'thought': True, 'movie': True, 'bad': True, 'kidding': True, 'great': True, ('test', 'review'): True, ('review', 'thought'): True, ('thought', 'movie'): True, ('movie', 'bad'): True, ('bad', 'kidding'): True, ('kidding', 'great'): True}\n"
     ]
    }
   ],
   "source": [
    "text = \"This is our test review because we thought the movie was very bad, just kidding it was great!\"\n",
    "words = word_tokenize(text.lower())\n",
    "\n",
    "print (\"Tokenized text:\", words, \"\\n\")\n",
    " \n",
    "print (\"Bigrammed text: \", bigram_words(words), \"\\n\")\n",
    "\n",
    "words_clean = clean_words(words, stopwords)\n",
    "print (\"Cleaned text: \" , words_clean, \"\\n\")\n",
    "\n",
    "print (\"All words cleaned: \", clean_all_words(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the NLTK movie_review corpus\n",
    "\n",
    "We use the nltk movie_review corpus to get two sepparate lists of bad and negative sample reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All negative and positive reviews in the NLTK movie_reviews corpus are added to sepparate lists\n",
    "pos_reviews = []\n",
    "for fileid in movie_reviews.fileids('pos'):\n",
    "    words = movie_reviews.words(fileid)\n",
    "    pos_reviews.append(words)\n",
    "\n",
    "neg_reviews = []\n",
    "for fileid in movie_reviews.fileids('neg'):\n",
    "    words = movie_reviews.words(fileid)\n",
    "    neg_reviews.append(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clean these lists using our clean_all_words() function that we created earlier and check to see how many reviews each list has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of positive/negative reviews in the NLTK movie_review corpus:  1000 1000\n"
     ]
    }
   ],
   "source": [
    "# Clean all the words in the review lists, using the function that we created earlier\n",
    "pos_reviews_set = []\n",
    "for words in pos_reviews:\n",
    "    pos_reviews_set.append((clean_all_words(words), 'pos'))\n",
    "\n",
    "neg_reviews_set = []\n",
    "for words in neg_reviews:\n",
    "    neg_reviews_set.append((clean_all_words(words), 'neg'))\n",
    "    \n",
    "print (\"Amount of positive/negative reviews in the NLTK movie_review corpus: \", len(pos_reviews_set), len(neg_reviews_set)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the training and test sets\n",
    "Now that we have the two lists of cleaned negative and positive reviews we will shuffle the lists to get a random order and create training and test sets of them. We will pass 25% of both negative and positive reviews into our test_set, the remaining 75% will be used for our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n"
     ]
    }
   ],
   "source": [
    "# We use the random shuffle function to randomize the lists to get different accuracy results everytime the program runs\n",
    "shuffle(pos_reviews_set)\n",
    "shuffle(neg_reviews_set)\n",
    " \n",
    "# We take 25% of the review lists to test data with and we use the\n",
    "#remaining data to train the NaivesBaysClassifier\n",
    "test_set = pos_reviews_set[:250] + neg_reviews_set[:250]\n",
    "train_set = pos_reviews_set[250:] + neg_reviews_set[250:]\n",
    "print(len(train_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the NaiveBayesClassifier\n",
    "We chose to use the NaiveBayesClassifier for our sentiment analysis because it was recommended all over the internet as the best one to be used for movie reviews.\n",
    "\n",
    "We initialize our *classifier* by passing the training data into it and then try out our accuracy with the test data.\n",
    "\n",
    "We also use the classifiers *show_most_informative_features()* to determine which features it finds the most effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current accuracy:  0.778\n",
      "Most Informative Features\n",
      "             outstanding = True              pos : neg    =     13.9 : 1.0\n",
      "    ('nothing', 'short') = True              pos : neg    =     12.3 : 1.0\n",
      "                  finest = True              pos : neg    =     11.8 : 1.0\n",
      "        ('one', 'worst') = True              neg : pos    =     11.8 : 1.0\n",
      "                  turkey = True              neg : pos    =     11.7 : 1.0\n",
      "       ('well', 'worth') = True              pos : neg    =     11.0 : 1.0\n",
      "     ('saving', 'grace') = True              neg : pos    =     10.3 : 1.0\n",
      "                 idiotic = True              neg : pos    =     10.2 : 1.0\n",
      "('completely', 'different') = True              pos : neg    =      9.7 : 1.0\n",
      "     ('bad', 'dialogue') = True              neg : pos    =      9.7 : 1.0\n",
      "         ('new', 'year') = True              neg : pos    =      9.7 : 1.0\n",
      "                    jedi = True              pos : neg    =      9.0 : 1.0\n",
      "             fascination = True              pos : neg    =      9.0 : 1.0\n",
      "         ('makes', 'us') = True              pos : neg    =      9.0 : 1.0\n",
      "                invasion = True              pos : neg    =      9.0 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# We initialize the classifier and use our training set with it\n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    "# TODO: test other classifiers\n",
    "\n",
    "# We use the nltk accuracy classifier on our test set to test the\n",
    "accuracy = classify.accuracy(classifier, test_set)\n",
    "print(\"Current accuracy: \", accuracy)\n",
    "print (classifier.show_most_informative_features(15)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting the probability\n",
    "Finally we create our function that determines if our review is positive, negative or mixed. It takes a string (in this case a review) as the arguement. It then tokenizes the string to be able to use every word uniquely and passes the tokenized text into our *clean_all_words()* function.\n",
    "\n",
    "After being cleaned of all stopwords and punctuations it uses the classifier with the trained probability data to determine if the review is either positive, negative of mixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This review is negative.\n",
      "0.999\n",
      "0.001\n",
      "This review is mixed.\n",
      "0.441\n",
      "0.559\n",
      "This review is positive.\n",
      "0.01\n",
      "0.99\n"
     ]
    }
   ],
   "source": [
    "# We test our program with a custom review\n",
    "# We used Matt Damon, who for some reason has a really high positive to negative ratio.\n",
    "bad_review = \"This movie is really bad and we would not recommend this movie even to our worst enemy. It really sucked.\"\n",
    "mixed_review = \"I don't know what to think about this movie, it was a bit weird but good, I kind of liked it, just kidding it's bad bad bad bad.\"\n",
    "good_review = \"Matt Damon\"\n",
    "\n",
    "def count_probability(review): \n",
    "    review_tokens = word_tokenize(review)\n",
    "    review_set = clean_all_words(review_tokens)\n",
    "    prob_result = classifier.prob_classify(review_set)\n",
    "    prob_result\n",
    "    if prob_result.prob(\"neg\") >= 0.70:\n",
    "        print('This review is negative.')\n",
    "    elif prob_result.prob(\"neg\") >= 0.30:\n",
    "        print('This review is mixed.')\n",
    "    else:\n",
    "        print('This review is positive.')\n",
    "    #print (type(prob_result.prob(\"neg\")))\n",
    "    print (round(prob_result.prob(\"neg\"), 3))\n",
    "    print (round(prob_result.prob(\"pos\"), 3))\n",
    "\n",
    "count_probability(bad_review)\n",
    "count_probability(mixed_review)\n",
    "count_probability(good_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using actual reviews\n",
    "\n",
    "We took a 1/10 and a 10/10 review from the movie IT, to test our actual accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1/10 review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This review is negative.\n",
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "review = \"\"\"It has become ritual for me to read the novel \"It\" once a year every year since it was released in 1986. The story is more than a gore-fest, it's a story about love and hope and friendship that is still meaningful to me to this day.\n",
    "\n",
    "The only thing this movie has in common with the beloved book, is its name and the characters names. IT is a literal disaster and a slap in the face to anyone who actually read and cherishes the book. There are NO character backstories, nor character development at all. You are literally thrust into the movie expecting to know everything about everyone and why they are the way they are. IE: Henry Bowers and why he hates the \"Losers Club\" - He LITERALLY starts the movie trying to kill them. This is sad, because a large portion of the novel was meticulously spent doing quite the opposite and made you relate to and fall in love with the characters.\n",
    "\n",
    "Editing? What editing? This is the worst edited movie I've ever seen in my life and I've seen a lot in 41 years. It was literally like the film makers shot 100 scenes, put the film in a hat, and took out said scenes and spliced them together at total random. I can't describe it any other way than saying, at one point, one of the characters (I can't tell who, because they all share the EXACT same personality) says, \"I banged your mom last night\", or something similar, and before the audience can even react, the scene changes to a jump scare happening in ANOTHER PART OF TOWN INSTANTLY and with no rhyme or reason. You don't have time to laugh at jokes, because they aren't funny (unlike Stephen King's jokes in the book) - and you don't have time to be scared, because you're still trying to process the dick joke that was still being told when the scene abruptly ended.\n",
    "\n",
    "While the filming location for the town of Derry was suitable, having the movie take place in the 1980's instead of the 1950's JUST TO APPEASE the \"Stranger Things\" crowd was simply a terrible decision. The 1950's were a totally different time, and much of the characters' reasoning and mannerisms that you need to make this movie work are lost to a time and cultural difference. These guys call themselves \"THE LOSERS SQUAD\" in this movie for god's sake! Kids didn't start calling themselves a \"squad\" until the 1980's (IE \"The Monster Squad) So, you love the book like me and are still reading? Thank you! Now let me list just SOME things that we both LOVE about the book that you will NOT find anywhere in this movie: The Deadlights, The Ritual of CHUD, The Mummy and the bridge, The Loser's Club Dam in the Barrens, the moving picture book (now its a slide machine), The Smokehouse, \"This is battery acid\", The Werewolf, Making the silver bullet after a game of monopoly, The stand pipe, Bower's hair turning white, \"beep beep Richie\", the giant bird, the 50's racism against Mike (actually Mike Hanlon himself is missing. The writers just made arguably the most important character an afterthought in this movie), character backstories, \"Hi Ho Silver-AWAY!\", Haystack... I could go on and on and on.\n",
    "\n",
    "With god awful editing, absolutely no character backstories, cheap teen jump scares, not being faithful to the book, and too much CGI usage: Simply put - if you want to know how this movie is like the book, read the first 10 pages of \"IT\", and burn the other 1077 pages because that is exactly what the screenwriter and director did to this failed abortion.\"\"\"\n",
    "count_probability(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10/10 review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This review is positive.\n",
      "0.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "review = \"\"\"The first sequence is classical example of US horror film. In which, the evil clown suddenly and naturally appears, and it victimizes the kid who drops his toys into drainage passage on a rainy day in the local city of US. \n",
    "\n",
    "We not only easily grab the setting of the evil clown and what he wants. It is a typical bogie man story who eats and kidnap kids who are afraid of it. \n",
    "\n",
    "It(2017) proves effective use of VFX. CG animation is to crate what traditional acting or camera working or editing technique can not achieve. \n",
    "\n",
    "This film actually avoided overused VFX and CG animations (CG is not an editorial work but it's computer animation). \n",
    "\n",
    "Moreover, it recovered or tried to revive the core spirit of 1980s' Goonies(1985) or Stand By Me(1986) to the certain extent. This is also well accepted and understandable. \n",
    "\n",
    "The evil clown is typical Boogieman like Freddy in A Nightmare on Elm Street (1984), and it has common feature with Freddy. Furthermore, it is scarier than the latter. \n",
    "\n",
    "I appreciate the legitimate and efficient structure of this film. It is the best horror film of 2017!\"\"\"\n",
    "count_probability(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result\n",
    "\n",
    "When testing our data with a few custom reviews there were a few problems, because every time we shuffle the lists of negative of positive reviews our test data will have a slightly different understanding of what is negative and what is positive. However, when we used the acutal reviews, which naturally were longer text, we got very accurate results and were satisfied with our program. The only thing that was hard to achieve was for the program to tell us if a long review was mixed.\n",
    "\n",
    "We found this project to be a good learning experience and got a good understand of what the NLTK corpus actually can be used for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
